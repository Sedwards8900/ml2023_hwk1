{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pandas\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question/Problem description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Problem description:\n",
    "\n",
    "For this problem, you need to download the Breast Cancer dataset from course webpage. \n",
    "The description of this dataset is in https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(original). \n",
    "I have removed the records with missing values for you. Here, you will obtain the learning curves (accuracy vs. \n",
    "training data size). Implement a logistic regression classifier with the assumption that each attribute value \n",
    "for a particular record is independently generated. You should submit the code electronically to iCollege.\n",
    "\n",
    "1.\t(10 points) Briefly describe how you implement it by giving the pseudocode. The pseudocode must include \n",
    "equations for estimating the classification parameters and for classifying a new example. Re- member, this \n",
    "should not be a printout of your code, but a high-level outline.\n",
    "\n",
    "2.  (15 points) Plot a learning curve: the accuracy vs. the size of the training data. Generate six points on \n",
    "the curve, using [.01 .02 .03 .125 .625 1] fractions of your training set and testing on the full test set each \n",
    "time. Average your results over 5 random splits of the data into a training and test set (always keep 2/3 of \n",
    "the data for training and 1/3 for testing, but randomize over which points go to training set and which to testing). \n",
    "This averaging will make your results less dependent on the order of records in the file. Specify your choice of \n",
    "regularization parameters and keep those parameters constant for these tests. A typical choice of constants would \n",
    "be Î» = 0 (no regularization).\n",
    "\n",
    "Attribute Information:\n",
    "\n",
    "1. Sample code number: id number\n",
    "2. Clump Thickness: 1 - 10\n",
    "3. Uniformity of Cell Size: 1 - 10\n",
    "4. Uniformity of Cell Shape: 1 - 10\n",
    "5. Marginal Adhesion: 1 - 10\n",
    "6. Single Epithelial Cell Size: 1 - 10\n",
    "7. Bare Nuclei: 1 - 10\n",
    "8. Bland Chromatin: 1 - 10\n",
    "9. Normal Nucleoli: 1 - 10\n",
    "10. Mitoses: 1 - 10\n",
    "11. Class: (2 for benign, 4 for malignant)\n",
    "\n",
    "\n",
    "In summary, what we need to get:\n",
    "\n",
    "    - Obtain learning curves (accuracy vs training data size)\n",
    "    - Implement Logistic Regression Classifier\n",
    "\t        Assume -> attribute values for particular record = independently generated\n",
    "    - Equations for estimating the classification parameters\n",
    "    - Equations for classifying a new example\n",
    "\n",
    "Please note, code development was based on tutorial at https://realpython.com/logistic-regression-python/\n",
    "\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Access data including number of attributes, total number of samples - X values, total number of Y values, all in the form of a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys:  dict_keys(['__header__', '__version__', '__globals__', 'data'])\n",
      "sample#:  683\n",
      "attributes#:  9\n",
      "input data [[ 5  1  1 ...  3  1  1]\n",
      " [ 5  4  4 ...  3  2  1]\n",
      " [ 3  1  1 ...  3  1  1]\n",
      " ...\n",
      " [ 5 10 10 ...  8 10  2]\n",
      " [ 4  8  6 ... 10  6  1]\n",
      " [ 4  8  8 ... 10  4  1]]\n",
      "shape:  (683, 9)\n",
      "type:  uint8  and:  <class 'numpy.ndarray'>\n",
      "type:  uint8  and:  <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#Load the data file\n",
    "mat=scipy.io.loadmat('data_breastcancer.mat')\n",
    "# print(\"full data: \", mat['data'])\n",
    "print(\"keys: \", mat.keys())\n",
    "\n",
    "#Number of samples\n",
    "n = mat['data']['n'][0][0][0][0]\n",
    "print(\"sample#: \", n)\n",
    "\n",
    "#Number of attributes\n",
    "d = mat['data']['d'][0][0][0][0]\n",
    "print(\"attributes#: \", d)\n",
    "\n",
    "#Input data - independent variables\n",
    "X = mat['data']['X'][0][0]\n",
    "print(\"input data\", X)\n",
    "print(\"shape: \", X.shape)\n",
    "print(\"type: \", X.dtype, \" and: \", type(X[0]))\n",
    "\n",
    "#Output labels - dependent variables\n",
    "Y = mat['data']['Y'][0][0]\n",
    "# print(\"output labels: \", Y)\n",
    "print(\"type: \", Y.dtype, \" and: \", type(X[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Split test and training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the train test split function from sklearn to split data\n",
    "x_train, x_test,y_train, y_test = train_test_split(X,Y,random_state=104, test_size=0.25, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Create necessary functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear rule - Vector notation function representing the weights, inputs and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Linear function f(x) = b0 + b1x1 + ... + brxr, ALSO CALLED THE LOGIT\n",
    "Variables b0, b1, ..., br are the estimators of the regression \n",
    "    coefficients -> predicted weights (w) or just coefficients\n",
    "\n",
    "Returns:\n",
    "    Predicted Y values as an np.array in the form of linear shape\n",
    "'''\n",
    "def linear_rule(weights:np.array, inputs: np.array, b:float, n:int, d:int) -> np.array:\n",
    "    # Create empty np.array for all predicted y values in linear form\n",
    "    y_pred_arr = np.zeros((n,1))\n",
    "    # print(y_pred_arr)\n",
    "    # navigate through each array of x values and add them up after multiplication with weights\n",
    "    for i in range(n):\n",
    "        sum_x = 0\n",
    "        for i in range(d):\n",
    "            sum_x += np.sum(inputs[i]*weights[i])\n",
    "        # print(\"sum_x: \", sum_x)\n",
    "        \n",
    "        # calculate individual y value for said array of x inputs\n",
    "        y_pred_arr[i]= sum_x + b\n",
    "\n",
    "    # Return array of predicted y values for each sample\n",
    "    return y_pred_arr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation fn -> sigmoid function for your binary logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The logistic regression function p(x) is the sigmoid function of f(x): p(x) = 1 / (1 + exp(-f(x))\n",
    "Result = Close to either 1 or 0.\n",
    "Interpreted as  as predicted probability that output for given x is 1. So, 1-p(x) = probability output is 0.\n",
    "\n",
    "Returns:\n",
    "    Sigmoid value\n",
    "'''\n",
    "def sigmoid_fn(fn_vals:np.array, n:int, d:int)->np.array:\n",
    "    sigm_arr = np.zeros((n,1))\n",
    "    for i in range(n):\n",
    "        sigm_arr[i] = 1/(1+math.exp(-fn_vals[i]))\n",
    "    return sigm_arr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSE - Mean squared error function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "MSE = average squared difference between estimated values and the actual value of y.\n",
    "Given as MSE = ((summation(i=1) of (predicted_yi - actual_yi)^2) / n\n",
    "\n",
    "Returns:\n",
    "    MSE float value for all predicted Ys\n",
    "    \n",
    "'''\n",
    "def mse_fn(y_predic_arr:np.array, y_vals:np.array, n:int) -> float:\n",
    "    y_ave = 0\n",
    "    for i in range(n):\n",
    "        y_ave += (y_predic_arr[i] - y_vals[i])**2\n",
    "    # Divide summation by the number of elements\n",
    "    return y_ave/n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update weights function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(weights:np.array, mse_val:float, d:int)-> np.array:\n",
    "    new_w = np.zeros((d, 1))\n",
    "    \n",
    "    return new_w"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Function that implements the first order iterative algorithm for finding\n",
    "a local minimum of differentiable function -> gradient descent\n",
    "\n",
    "Returns:\n",
    "    Next theta parameter value\n",
    "'''\n",
    "def gradient_descent(x_input:np.array, learn_rate) -> np.array:\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Get best weights for all observations via MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Use the arg max of the log-likelihood function l(w) = log L(w) for all observations i = 1 through n\n",
    "AKA: Get the MLE for our observations -> summation of probabilitites for each observation\n",
    "MLE function = l(w) = Summation(i=1) ln (P(Y=1 | Xi, W)) + (1-Yi)* ln (P(Y= | Xi, w))\n",
    "MLE function = l(w) = Summation(i=1) Yi*(W0 + Summ(j=1) wjxj^i) - ln (1 + exp (W0 + Summ(j=1) wjxj^i))\n",
    "\n",
    "returns:\n",
    "    MLE\n",
    "'''\n",
    "def mle_fn(Yn, Xn, weights, X, w0):\n",
    "    sum_i = 0\n",
    "    sum_j = 0\n",
    "    # Calculate summation of w0 + the summation of weights * x values ^ i\n",
    "    for i in range(Yn):\n",
    "        for j in range(Xn):\n",
    "            sum_j += weights[j]*X[j]**i\n",
    "        sum_i += Y[i]*(w0 + sum_j) - math.log(1 + math.exp(w0 + sum_j))\n",
    "    \n",
    "    # Return total MLE\n",
    "    return sum_i\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initiate values for parameters that need adjustment as well as others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights:  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "b:  0\n"
     ]
    }
   ],
   "source": [
    "# Initiate weights and b value so they can be adjusted - the slope of the line\n",
    "weights = np.zeros(d)\n",
    "print(\"weights: \", weights)\n",
    "\n",
    "# b is the intercept, initiated to 0\n",
    "b = 0\n",
    "print(\"b: \", b)\n",
    "\n",
    "# Select number of iterations for gradient descent\n",
    "iterations = 150\n",
    "\n",
    "# Select learning rate - can be set between 0.1 and 0.0001\n",
    "learn_r = 0.1\n",
    "\n",
    "# Variable to store predictions\n",
    "y_pred = np.zeros(n)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proceed to use functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use linear classification function\n",
    "z = linear_rule(weights, x_train, b, n, d)\n",
    "# print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then make it a classification problem by using sigmoid function\n",
    "h = sigmoid_fn(z, n, d)\n",
    "# print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25]\n"
     ]
    }
   ],
   "source": [
    "# Calculate mean square error\n",
    "mse = mse_fn(h, Y, n)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "# Update weights after having calculated the average\n",
    "weights = update_weights(weights, mse, d)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(iterations):\n",
    "    weights = update_weights(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # recall functions\n",
    "# z = linear_classification_rule(weights, X, w0)\n",
    "# h = sigmoid_fn(z)\n",
    "# c = mle_fn(len(Y), len(X), weights, X, w0)\n",
    "\n",
    "# cost = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Logistic regression determines the best predicted weights ðâ, ðâ, â¦, ðáµ£ such that \n",
    "the function ð(ð±) is as close as possible to all actual responses ð¦áµ¢, ð = 1, â¦, ð, \n",
    "where ð is the number of observations. \n",
    "\n",
    "The process of calculating the best weights using available observations is called \n",
    "model training or fitting.\n",
    "\n",
    "Proceed to fit the X and Y values into the model by using .fit() function, which \n",
    "takes x and y. The returned value is the model instance.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input data 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"input data\", X[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the best weights, you usually maximize the log-likelihood function (LLF) \n",
    "# for all observations ð = 1, â¦, ð. This method is called the maximum likelihood \n",
    "# estimation and is represented by the equation LLF = Î£áµ¢(ð¦áµ¢ log(ð(ð±áµ¢)) + (1 â ð¦áµ¢) log(1 â ð(ð±áµ¢)))."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
